# Datasets Directory

This directory contains training datasets for the Paninian Engine.

## toy_dataset.jsonl

**Purpose**: Proof of Concept (POC) dataset for first training experiment

**Generated by**: `scripts/generate_toy_dataset.py`

**Contents**:
- 53 English-to-Sanskrit sentence pairs
- Complete grammatical breakdowns (Karaka analysis)
- Grammar notes explaining case endings and verb forms
- Ready for Stage 2 (Karaka) training

### Dataset Statistics

- **Total examples**: 53
- **Complexity distribution**:
  - Simple: 24 examples
  - Medium: 23 examples
  - Complex: 6 examples

### Sentence Types

1. Simple Subject-Verb (Karta + Kriya)
2. Subject-Verb-Object (Karta + Karma + Kriya)
3. With Location (Adhikarana)
4. With Recipient (Sampradana)
5. With Source (Apadana)
6. With Instrument (Karana)
7. Plural subjects and objects

### Format

Each line is a JSON object with:
- `id`: Example number
- `instruction`: Translation task description
- `input`: English sentence
- `output`: Sanskrit translation (transliterated)
- `output_devanagari`: Sanskrit in Devanagari (placeholder)
- `karaka`: Complete grammatical breakdown
- `grammar_notes`: Human-readable grammar explanation
- `stage`: Training stage ("karaka")
- `complexity`: Sentence complexity level

### Usage

```bash
# View dataset
cat datasets/toy_dataset.jsonl | jq .

# Count examples
wc -l datasets/toy_dataset.jsonl

# View a specific example
sed -n '6p' datasets/toy_dataset.jsonl | jq .
```

### Next Steps

1. **Review dataset**: Verify all translations follow Panini's rules
2. **Run training experiment**: Use this dataset for initial POC training
3. **Validate grammar**: Check case endings (Vibhakti) are correct
4. **Scale up**: Generate larger datasets using the same approach

---

*Datasets Directory - Project Panini*  
*Last Updated: January 16, 2026*
