# Project Panini: A Neuro-Symbolic Approach to Low-Resource Sanskrit Modeling

**Date:** January 16, 2026  
**Project Lead:** Raghavendra V. Karnam  
**Repository:** [Insert GitHub Link Here]  
**Status:** Request for Comment (RFC) / Pre-Alpha

---

## 1. Abstract

The current paradigm of Large Language Model (LLM) training relies on massive, indiscriminately scraped datasets and prohibitively expensive compute clusters, often costing millions of dollars. While effective for high-resource languages like English, this brute-force statistical approach is inefficient for highly structured languages like Sanskrit. This paper introduces **Project Panini**, an initiative to develop an open-source, high-fidelity Small Language Model (SLM) for Sanskrit achievable on consumer-grade hardware (minimal GPU constraints).

Our core hypothesis is that Sanskrit's unique linguistic architecture—specifically the generative rules codified in Panini's Ashtadhyayi (c. 500 BCE)—allows for a neuro-symbolic training methodology. Rather than relying solely on probabilistic next-token prediction from sparse web data, we propose a **dual-stream data strategy**:

1. **Deterministic Synthetic Data**: Generated programmatically using Panini's algorithms to enforce perfect grammatical structure.
2. **Stylistic Fine-Tuning**: Utilizing the classical corpus of Kalidasa and select modern texts to capture aesthetic and semantic nuance.

By injecting these grammatical priors into efficient architectures (e.g., Mistral-7B, Llama-3-8B) via Low-Rank Adaptation (LoRA) and 4-bit quantization, we aim to drastically reduce the "sample complexity" required for fluency. This approach shifts the burden from massive compute to high-quality data engineering. We invite the academic and open-source community to collaborate on this "grammatically grounded" architecture, demonstrating that deeply structured languages can be modeled efficiently without industrial-scale resources.

**Keywords:** Small Language Models (SLM), Neuro-Symbolic AI, Sanskrit, Low-Rank Adaptation (LoRA), Panini, Synthetic Data Generation.

---

## 2. Problem Statement

Standard LLMs treat language as a probability distribution. To learn grammar, they must see billions of tokens to statistically approximate rules that—in Sanskrit—are already strictly defined. Training a model from scratch to learn these rules requires millions of dollars in GPU compute. This barrier prevents academic experimentation and localized deployment.

---

## 3. Proposed Solution

We propose a **Neuro-Symbolic Architecture**. We will not ask the neural network to "guess" the grammar; we will "teach" it the grammar using synthetic data generated by a Symbolic Rule Engine (based on Panini's Ashtadhyayi). This allows us to use a much smaller model (7B parameters or fewer) while maintaining high grammatical accuracy.

---

## 4. High-Level System Architecture

The system follows a three-stage pipeline: **The Data Factory**, **The Trainer**, and **The Inference Edge**.

### Phase A: The Data Factory (Symbolic Engine)

**Objective:** Generate infinite, grammatically perfect Sanskrit text.

**Core Technology:** Python scripts calling Rust-based libraries (e.g., Vidyut or Dhatupatha parsers).

**Pipeline:**

1. **Root Selection**: Select verbal roots (dhatu) and nominal bases (pratipadika).
2. **Rule Application**: Apply Paninian rules for derivation (e.g., Sandhi, Subanta, Tinganta).
3. **Output**: A dataset of JSONL pairs:
   ```json
   {
     "input": "Root + Grammar Rules",
     "output": "Perfectly Formed Sentence"
   }
   ```

### Phase B: The Trainer (Neural Engine)

**Objective:** Teach the model to generalize style and meaning.

**Core Technology:** Unsloth (for efficient training) + PyTorch.

**Hardware Target:** Single NVIDIA RTX 3090/4090 (24GB VRAM).

**Methodology:**

1. **Base Model**: Load Mistral-7B or Llama-3-8B (Quantized to 4-bit).
2. **LoRA Adapters**: Train only 1-2% of the model parameters (the "Adapter" layers) on the synthetic data.
3. **Curriculum Learning**: Start with simple grammar (Synthetic), move to complex literature (Kalidasa), end with modern usage (News/Wiki).

### Phase C: Inference (Deployment)

**Objective:** Run the model on a laptop.

**Core Technology:** llama.cpp (GGUF format).

**Interface:** Simple Python web app (Streamlit) or Terminal CLI.

---

## 5. Roadmap & Student Engagement

We are seeking contributors for the following initial phases:

### Phase 1: Data Engineering

Building the Python wrappers for Paninian rule engines to generate the "Golden Dataset."

### Phase 2: Scraping & Cleaning

Processing raw text files of Meghaduta and Raghuvamsha, cleaning XML tags, and formatting for tokenization.

### Phase 3: Model Training

Running initial LoRA experiments on Google Colab (Free Tier) or local GPUs.

---

*Project Panini - Neuro-Symbolic AI for Sanskrit*  
*Last Updated: January 16, 2026*
